{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697f6aec",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d587a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import string as st\n",
    "import re\n",
    "import nltk\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959eb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('C:/Users/jerry/Work/NLP/data/spam_text_message_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee780ca",
   "metadata": {},
   "source": [
    "## Text cleaning and processing steps:\n",
    "> Remove punctuations <br>\n",
    "> Convert text to tokens<br>\n",
    "> Remove tokens of length less than or equal to 3 <br>\n",
    "> Remove stopwords using NLTK corpus stopwords list to match <br>\n",
    "> Apply stemming <br>\n",
    "> Apply lemmatization <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_punc'] = data['Message'].apply(lambda x: remove_punct(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ed1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
    "    on special characters, tabs or any other string based on which text is to be seperated into tokens.\n",
    "'''\n",
    "def tokenize(text):\n",
    "    text = re.split('\\s+' ,text)\n",
    "    return [x.lower() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['removed_punc'].apply(lambda msg : tokenize(msg))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens of length less than 3\n",
    "def remove_small_words(text):\n",
    "    return [x for x in text if len(x) > 3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['larger_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
    "    list could be created and used to limit the matches in input text. \n",
    "'''\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_tokens'] = data['larger_tokens'].apply(lambda x : remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to get root words \n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stem_words'] = data['clean_tokens'].apply(lambda wrd: stemming(wrd))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69846f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization on tokens\n",
    "def lemmatize(text):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    return [word_net.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82aec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_words'] = data['clean_tokens'].apply(lambda x : lemmatize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a945d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentences to get clean text as input for vectors\n",
    "\n",
    "def return_sentences(tokens):\n",
    "    return \" \".join([word for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['lemma_words'].apply(lambda x : return_sentences(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2da0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
